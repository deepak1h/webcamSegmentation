{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914bb152",
   "metadata": {},
   "outputs": [],
   "source": [
    "#requirments if not installed\n",
    "\n",
    "# !pip3 install torch torchvision --index-url https://download.pytorch.org/whl/cu118\n",
    "# !pip install opencv-python\n",
    "# !pip install pycocotools\n",
    "# !pip install cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4efb0a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "\n",
    "import torch\n",
    "import cv2\n",
    "from yolact import Yolact\n",
    "from collections import defaultdict\n",
    "import torch.backends.cudnn as cudnn\n",
    "from utils.augmentations import FastBaseTransform\n",
    "from utils import timer\n",
    "from data import cfg\n",
    "from layers.output_utils import postprocess\n",
    "from data import COLORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c78ad97",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    cudnn.fastest = True\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b00eb48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc=0.45\n",
    "top_k =15\n",
    "color_cache = defaultdict(lambda: {})\n",
    "display_masks=True;\n",
    "display_text_bbx=True;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcea431d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deepa\\anaconda3\\envs\\newtest\\lib\\site-packages\\torch\\jit\\_recursive.py:260: UserWarning: 'pred_layers' was found in ScriptModule constants,  but it is a non-constant submodule. Consider removing it.\n",
      "  warnings.warn(\"'{}' was found in ScriptModule constants, \"\n",
      "C:\\Users\\deepa\\anaconda3\\envs\\newtest\\lib\\site-packages\\torch\\jit\\_recursive.py:260: UserWarning: 'downsample_layers' was found in ScriptModule constants,  but it is a non-constant submodule. Consider removing it.\n",
      "  warnings.warn(\"'{}' was found in ScriptModule constants, \"\n",
      "C:\\Users\\deepa\\anaconda3\\envs\\newtest\\lib\\site-packages\\torch\\jit\\_recursive.py:260: UserWarning: 'lat_layers' was found in ScriptModule constants,  but it is a non-constant submodule. Consider removing it.\n",
      "  warnings.warn(\"'{}' was found in ScriptModule constants, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Done.\n"
     ]
    }
   ],
   "source": [
    "print('Loading model...', end='')\n",
    "net = Yolact()\n",
    "net.load_weights(\"model/yolact_base_54_800000.pth\")\n",
    "net.eval()\n",
    "net.cuda()\n",
    "print(' Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fea1d96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prep_display(dets_out, img, h, w, undo_transform=True, class_color=False, mask_alpha=0.45, fps_str=''):\n",
    "    \n",
    "    if undo_transform:\n",
    "        img_numpy = undo_image_transformation(img, w, h)\n",
    "        img_gpu = torch.Tensor(img_numpy).cuda()\n",
    "    else:\n",
    "        img_gpu = img / 255.0\n",
    "        h, w, _ = img.shape\n",
    "    \n",
    "    with timer.env('Postprocess'):\n",
    "        save = cfg.rescore_bbox\n",
    "        cfg.rescore_bbox = True\n",
    "        t = postprocess(dets_out, w, h, visualize_lincomb = False,\n",
    "                                        crop_masks        = True,\n",
    "                                        score_threshold   = 0)\n",
    "        cfg.rescore_bbox = save\n",
    "    \n",
    "\n",
    "    with timer.env('Copy'):\n",
    "        idx = t[1].argsort(0, descending=True)[:top_k]\n",
    "        \n",
    "        if cfg.eval_mask_branch:\n",
    "            # Masks are drawn on the GPU, so don't copy\n",
    "            masks = t[3][idx]\n",
    "        classes, scores, boxes = [x[idx].cpu().numpy() for x in t[:3]]\n",
    "\n",
    "    num_dets_to_consider = min(top_k, classes.shape[0])\n",
    "    for j in range(num_dets_to_consider):\n",
    "        if scores[j] < sc:\n",
    "            num_dets_to_consider = j\n",
    "            break\n",
    "\n",
    "    def get_color(j, on_gpu=None):\n",
    "        global color_cache\n",
    "        color_idx = (classes[j] * 5 if class_color else j * 5) % len(COLORS)\n",
    "        \n",
    "        if on_gpu is not None and color_idx in color_cache[on_gpu]:\n",
    "            return color_cache[on_gpu][color_idx]\n",
    "        else:\n",
    "            color = COLORS[color_idx]\n",
    "            if not undo_transform:\n",
    "                # The image might come in as RGB or BRG, depending\n",
    "                color = (color[2], color[1], color[0])\n",
    "            if on_gpu is not None:\n",
    "                color = torch.Tensor(color).to(on_gpu).float() / 255.\n",
    "                color_cache[on_gpu][color_idx] = color\n",
    "            return color\n",
    "\n",
    "    if display_masks and cfg.eval_mask_branch and num_dets_to_consider > 0:\n",
    "        # After this, mask is of size [num_dets, h, w, 1]\n",
    "        masks = masks[:num_dets_to_consider, :, :, None]\n",
    "        colors = torch.cat([get_color(j, on_gpu=img_gpu.device.index).view(1, 1, 1, 3) for j in range(num_dets_to_consider)], dim=0)\n",
    "        masks_color = masks.repeat(1, 1, 1, 3) * colors * mask_alpha\n",
    "\n",
    "        inv_alph_masks = masks * (-mask_alpha) + 1\n",
    "\n",
    "        masks_color_summand = masks_color[0]\n",
    "        if num_dets_to_consider > 1:\n",
    "            inv_alph_cumul = inv_alph_masks[:(num_dets_to_consider-1)].cumprod(dim=0)\n",
    "            masks_color_cumul = masks_color[1:] * inv_alph_cumul\n",
    "            masks_color_summand += masks_color_cumul.sum(dim=0)\n",
    "\n",
    "        img_gpu = img_gpu * inv_alph_masks.prod(dim=0) + masks_color_summand\n",
    "\n",
    "    img_numpy = (img_gpu * 255).byte().cpu().numpy()\n",
    "\n",
    "    if num_dets_to_consider == 0:\n",
    "        return img_numpy\n",
    "\n",
    "    if display_text_bbx:\n",
    "        for j in reversed(range(num_dets_to_consider)):\n",
    "            x1, y1, x2, y2 = boxes[j, :]\n",
    "            color = get_color(j)\n",
    "            score = scores[j]\n",
    "\n",
    "            if True:\n",
    "                cv2.rectangle(img_numpy, (x1, y1), (x2, y2), color, 1)\n",
    "                \n",
    "                _class = cfg.dataset.class_names[classes[j]]\n",
    "                text_str = '%s: %.2f' % (_class, score) if True else _class\n",
    "\n",
    "                font_face = cv2.FONT_HERSHEY_DUPLEX\n",
    "                font_scale = 0.6\n",
    "                font_thickness = 1\n",
    "\n",
    "                text_w, text_h = cv2.getTextSize(text_str, font_face, font_scale, font_thickness)[0]\n",
    "\n",
    "                text_pt = (x1, y1 - 3)\n",
    "                text_color = [255, 255, 255]\n",
    "\n",
    "                cv2.rectangle(img_numpy, (x1, y1), (x1 + text_w, y1 - text_h - 4), color, -1)\n",
    "                cv2.putText(img_numpy, text_str, text_pt, font_face, font_scale, text_color, font_thickness, cv2.LINE_AA)\n",
    "    \n",
    "    return img_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01a63ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateimg(img):\n",
    "    \n",
    "    frame = torch.from_numpy(img).cuda().float()\n",
    "\n",
    "    batch = FastBaseTransform()(frame.unsqueeze(0))\n",
    "    with torch.no_grad():\n",
    "        preds = net(batch)\n",
    "\n",
    "    res = prep_display(preds, frame, None, None, undo_transform=False)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcc98ec7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m ret, frame \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret:\n\u001b[1;32m----> 7\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mevaluateimg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Check if the frame was successfully captured\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ret:\n",
      "Cell \u001b[1;32mIn[6], line 7\u001b[0m, in \u001b[0;36mevaluateimg\u001b[1;34m(img)\u001b[0m\n\u001b[0;32m      5\u001b[0m batch \u001b[38;5;241m=\u001b[39m FastBaseTransform()(frame\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m----> 7\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m res \u001b[38;5;241m=\u001b[39m prep_display(preds, frame, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, undo_transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\newtest\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\Desktop\\project\\ML\\yolact\\yolact\\yolact.py:678\u001b[0m, in \u001b[0;36mYolact.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    676\u001b[0m         pred_outs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconf\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(pred_outs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconf\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 678\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_outs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\project\\ML\\yolact\\yolact\\layers\\functions\\detection.py:71\u001b[0m, in \u001b[0;36mDetect.__call__\u001b[1;34m(self, predictions, net)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_size):\n\u001b[0;32m     70\u001b[0m     decoded_boxes \u001b[38;5;241m=\u001b[39m decode(loc_data[batch_idx], prior_data)\n\u001b[1;32m---> 71\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoded_boxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minst_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m proto_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     74\u001b[0m         result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproto\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m proto_data[batch_idx]\n",
      "File \u001b[1;32m~\\Desktop\\project\\ML\\yolact\\yolact\\layers\\functions\\detection.py:103\u001b[0m, in \u001b[0;36mDetect.detect\u001b[1;34m(self, batch_idx, conf_preds, decoded_boxes, mask_data, inst_data)\u001b[0m\n\u001b[0;32m    101\u001b[0m         boxes, masks, classes, scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfast_nms(boxes, masks, scores, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnms_thresh, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtop_k)\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 103\u001b[0m     boxes, masks, classes, scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraditional_nms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnms_thresh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconf_thresh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_cross_class_nms:\n\u001b[0;32m    106\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWarning: Cross Class Traditional NMS is not implemented.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\Desktop\\project\\ML\\yolact\\yolact\\layers\\functions\\detection.py:209\u001b[0m, in \u001b[0;36mDetect.traditional_nms\u001b[1;34m(self, boxes, masks, scores, iou_threshold, conf_thresh)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    208\u001b[0m preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([boxes[conf_mask], cls_scores[:, \u001b[38;5;28;01mNone\u001b[39;00m]], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m--> 209\u001b[0m keep \u001b[38;5;241m=\u001b[39m \u001b[43mcnms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miou_threshold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    210\u001b[0m keep \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor(keep, device\u001b[38;5;241m=\u001b[39mboxes\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mlong()\n\u001b[0;32m    212\u001b[0m idx_lst\u001b[38;5;241m.\u001b[39mappend(idx[keep])\n",
      "File \u001b[1;32m~\\Desktop\\project\\ML\\yolact\\yolact\\utils\\cython_nms.pyx:36\u001b[0m, in \u001b[0;36mutils.cython_nms.nms\u001b[1;34m()\u001b[0m\n\u001b[0;32m     34\u001b[0m cdef int ndets = dets.shape[0]\n\u001b[0;32m     35\u001b[0m cdef np.ndarray[np.int_t, ndim=1] suppressed = \\\n\u001b[1;32m---> 36\u001b[0m         np.zeros((ndets), dtype=np.int)\n\u001b[0;32m     37\u001b[0m \n\u001b[0;32m     38\u001b[0m # nominal indices\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\newtest\\lib\\site-packages\\numpy\\__init__.py:284\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(attr)\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtesting\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tester\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Tester\n\u001b[1;32m--> 284\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    285\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;18m__name__\u001b[39m, attr))\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'int'"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if ret:\n",
    "        image = evaluateimg(frame)\n",
    "\n",
    "    # Check if the frame was successfully captured\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Display the captured frame on the screen\n",
    "    cv2.imshow(\"Camera Output\", image)\n",
    "\n",
    "    # Break the loop if the 'q' key is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "# Release the webcam and close all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37921b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ba5fc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11af102",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
